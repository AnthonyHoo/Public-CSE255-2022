{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 24 ms, total: 204 ms\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf=sc.getConf()\n",
    "# Conf.getAll()\n",
    "conf.get(\"spark.executor.instances\")\n",
    "conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local files\n",
    "\n",
    "### Download and Uplaod\n",
    "\n",
    "Download and upload files via the Spark Notebook interface.\n",
    "\n",
    "### Access Local Files\n",
    "\n",
    "The file path to local files requires `file://` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/etc/passwd\r\n"
     ]
    }
   ],
   "source": [
    "ls /etc/passwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# s3helper\n",
    "\n",
    "The object `s3helper` is a tool to transfer files between local filesystem, HDFS and S3.\n",
    "\n",
    "Run `s3helper.help()` to learn all its methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from boto.exception import S3ResponseError\n",
    "from boto.s3.connection import S3Connection\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _run_command(command, detail=False):\n",
    "    proc = subprocess.Popen(command.split(),\n",
    "                            stdout=subprocess.PIPE,\n",
    "                            stderr=subprocess.PIPE)\n",
    "    if detail:\n",
    "        while True:\n",
    "            line = proc.stdout.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line)\n",
    "    return proc.communicate()\n",
    "\n",
    "\n",
    "def _list_hdfs(path):\n",
    "    command = \"/usr/bin/hdfs dfs -ls %s\" % path\n",
    "    out, err = _run_command(command)\n",
    "    if out:\n",
    "        print(out)\n",
    "\n",
    "\n",
    "def _s3_to_hdfs(files, tgt):\n",
    "    out, err1 = _run_command(\"/usr/bin/hdfs dfs -mkdir -p %s\" % tgt)\n",
    "    if err1:\n",
    "        print(err1)\n",
    "        return\n",
    "    out, err2 = _run_command(\"/usr/bin/hdfs dfs -cp %s %s\" % (files, tgt))\n",
    "    if err2:\n",
    "        print(err2)\n",
    "\n",
    "\n",
    "def _hdfs_to_s3(files, tgt):\n",
    "    out, err = _run_command(\"/usr/bin/hadoop distcp %s %s\" % (files, tgt), True)\n",
    "    if err:\n",
    "        print(err)\n",
    "\n",
    "\n",
    "def _local_to_hdfs(src, tgt):\n",
    "    if tgt[0] != '/':\n",
    "        tgt = '/' + tgt\n",
    "    out, err1 = _run_command(\"/usr/bin/hdfs dfs -mkdir -p %s\" % tgt)\n",
    "    if err1:\n",
    "        print(err1)\n",
    "        return\n",
    "    out, err2 = _run_command(\"/usr/bin/hdfs dfs -cp %s %s\" % (\"file://\" + os.path.join(src, '*'),\n",
    "                                                              tgt))\n",
    "    if err2:\n",
    "        print(err2)\n",
    "\n",
    "\n",
    "class S3Helper:\n",
    "    \"\"\"A helper function to access S3 files\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conn = None\n",
    "        self.bucket_name = None\n",
    "        self.bucket = None\n",
    "\n",
    "    @staticmethod\n",
    "    def help():\n",
    "        print('''\n",
    "        s3helper is a helper object to move files and directory between\n",
    "        local filesystem, AWS S3 and local HDFS.\n",
    "\n",
    "        Usage:\n",
    "\n",
    "        1. Open a S3 bucket under your account\n",
    "            s3helper.open_bucket(<bucket_name>)\n",
    "        2. List all files under the opened S3 bucket\n",
    "            s3helper.ls() or s3helper.ls_s3()\n",
    "        Or optionally,\n",
    "            s3helper.ls(<file_path>) or s3helper.ls_s3(<file_path>)\n",
    "        3. List all files on HDFS\n",
    "            s3helper.ls_hdfs()\n",
    "        Or optionally,\n",
    "            s3helper.ls_hdfs(<file_path>)\n",
    "        where <file_path> is an absolute path in the opened S3 bucket.\n",
    "\n",
    "        Now you can access your S3 files.\n",
    "\n",
    "        1. Transfer files between S3 and HDFS\n",
    "          a. To download all S3 files under a directory to HDFS, please call\n",
    "                s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)\n",
    "          b. To upload a directory on HDFS to S3, please call\n",
    "                s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)\n",
    "\n",
    "        2. Transfer files between S3 and local filesystem (not HDFS)\n",
    "          a. To download one single S3 file to local filesystem, please call\n",
    "                s3helper.s3_to_local(<s3_file_path>, <local_file_path>)\n",
    "          b. To upload a file on local filesystem to S3, please call\n",
    "                s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)\n",
    "\n",
    "        3. Transfer files between local filesystem and HDFS\n",
    "          a. To upload a directory on local filesystem to HDFS, please call\n",
    "                s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)\n",
    "\n",
    "        4. Get S3 file paths without data transfer\n",
    "          a. To get the URLs of S3 files under a directory, please call\n",
    "                s3helper.get_path(<s3_directory_path>)\n",
    "          Note this method do nothing on your local HDFS.\n",
    "        ''')\n",
    "\n",
    "    def open_bucket(self, bucket_name, region=None):\n",
    "        \"\"\"Open a S3 bucket.\n",
    "\n",
    "            Args:\n",
    "                bucket_name\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        if bucket_name.startswith('s3n://') or bucket_name.startswith('s3://'):\n",
    "            raise ValueError('bucket_name must NOT contain any prefix '\n",
    "                             '(e.g. s3:// or s3n://)')\n",
    "\n",
    "        while bucket_name[-1] == '/':\n",
    "            bucket_name = bucket_name[:-1]\n",
    "        if region is None:\n",
    "            print(\"Warning: S3 region is not defined. Default region is set to 'us-east-1'.\")\n",
    "            region = \"us-east-1\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.conn = S3Connection(host=\"s3.{}.amazonaws.com\".format(region))\n",
    "        try:\n",
    "            self.bucket = self.conn.get_bucket(self.bucket_name)\n",
    "        except S3ResponseError as e:\n",
    "            self.bucket = None\n",
    "            self.bucket_name = None\n",
    "            print('Open S3 bucket \"%s\" failed.\\n' % bucket_name + str(e))\n",
    "            print(e.message)\n",
    "\n",
    "    def ls(self, path=''):\n",
    "        \"\"\"same as ls_s3\"\"\"\n",
    "        return self.ls_s3(path)\n",
    "\n",
    "    def ls_s3(self, path=''):\n",
    "        \"\"\"List all files in `path` on S3.\n",
    "\n",
    "            Args:\n",
    "                path\n",
    "            Returns:\n",
    "                an array of files in `path`\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('No bucket is opened. '\n",
    "                            'Please use open_bucket method first.')\n",
    "\n",
    "        path = path.strip()\n",
    "        if len(path) and path[0] == '/':\n",
    "            path = path[1:]\n",
    "        files = self.bucket.list(prefix=path)\n",
    "\n",
    "        if path == '':\n",
    "            k = 1\n",
    "        else:\n",
    "            k = len(path.split('/')) + 1\n",
    "        return sorted(list(set(\n",
    "            ['/'.join(t.key.split('/')[:k]) for t in files])))\n",
    "\n",
    "    @staticmethod\n",
    "    def ls_hdfs(path='/'):\n",
    "        \"\"\"List all files in `path` on HDFS.\"\"\"\n",
    "        if not path or path[0] != '/':\n",
    "            path = '/' + path\n",
    "        return _list_hdfs(path)\n",
    "\n",
    "    def get_path(self, path=''):\n",
    "        \"\"\"Get paths of all files in `path` with s3 prefix,\n",
    "           which can be passed to Spark.\n",
    "\n",
    "            Args:\n",
    "                path\n",
    "            Returns:\n",
    "                an array of file paths with s3 prefix\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('no bucket is opened.')\n",
    "\n",
    "        path = path.strip()\n",
    "        if len(path) and path[0] == '/':\n",
    "            path = path[1:]\n",
    "        files = self.bucket.list(prefix=path)\n",
    "        prefix = \"s3n://%s/\" % self.bucket_name\n",
    "        return [prefix + t.key for t in files]\n",
    "\n",
    "    def s3_to_hdfs(self, src, tgt):\n",
    "        \"\"\"Load all files in `src` to the directory `tgt` in HDFS.\n",
    "\n",
    "            Args:\n",
    "                src, tgt\n",
    "            Returns:\n",
    "                an array of file paths in HDFS\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('no bucket is opened.')\n",
    "\n",
    "        src, tgt = src.strip(), tgt.strip()\n",
    "        if len(src) and src[0] == '/':\n",
    "            src = src[1:]\n",
    "        if tgt == '' or (tgt[0] != '/' and not tgt.startswith('hdfs://')):\n",
    "            tgt = '/' + tgt\n",
    "        if tgt[-1] != '/':\n",
    "            tgt = tgt + '/'\n",
    "        files = self.bucket.list(prefix=src)\n",
    "        prefix = \"s3n://%s/\" % self.bucket_name\n",
    "        _s3_to_hdfs(' '.join([prefix + t.key for t in files]), tgt)\n",
    "        self.ls_hdfs(tgt)\n",
    "\n",
    "    def hdfs_to_s3(self, src, tgt):\n",
    "        \"\"\"Upload a directory `src` on HDFS to a directory `tgt` on S3.\n",
    "\n",
    "           Args:\n",
    "                src, tgt\n",
    "           Returns:\n",
    "                file list of the `tgt` directory on S3 after uploading\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('no bucket is opened. '\n",
    "                            'See help() method for more info')\n",
    "\n",
    "        src, tgt = src.strip(), tgt.strip()\n",
    "        if src == '' or (src[0] != '/' and not src.startswith('hdfs://')):\n",
    "            src = '/' + src\n",
    "        if src[-1] != '/':\n",
    "            src = src + '/'\n",
    "        if len(tgt) and tgt[0] == '/':\n",
    "            tgt = tgt[1:]\n",
    "        tgt = \"s3n://%s/\" % self.bucket_name + tgt\n",
    "        print(\"*NOTE*\\n\"\n",
    "              \"This method will create a MapReudce job to upload the content \"\n",
    "              \"in HDFS to S3. The process may take a while.\\n\\n\")\n",
    "        _hdfs_to_s3(src, tgt)\n",
    "\n",
    "    def local_to_s3(self, filename, tgt):\n",
    "        \"\"\"Save a local file `filename` to the directory `tgt` on S3.\n",
    "\n",
    "            Args:\n",
    "                filename, tgt\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('no bucket is opened.')\n",
    "        if not os.path.exists(filename):\n",
    "            raise Exception(\"File does not exist.\")\n",
    "        if os.path.isdir(filename):\n",
    "            raise Exception(\n",
    "                \"Transfer between S3 and local filesystem \"\n",
    "                \"does not support directory.\")\n",
    "\n",
    "        tgt = tgt.strip()\n",
    "        if len(tgt) and tgt[0] == '/':\n",
    "            tgt = tgt[1:]\n",
    "        if not tgt:\n",
    "            tgt = filename.rsplit('/', 1)[-1]\n",
    "        k = self.bucket.new_key(tgt)\n",
    "        k.set_contents_from_filename(filename)\n",
    "\n",
    "    def s3_to_local(self, src, tgt):\n",
    "        \"\"\"Download the remote file `key_name` on S3 to local.\n",
    "\n",
    "            Args:\n",
    "                src, tgt\n",
    "            Returns:\n",
    "                None\n",
    "        \"\"\"\n",
    "        if not self.bucket:\n",
    "            raise Exception('no bucket is opened.')\n",
    "\n",
    "        key = src.strip()\n",
    "        if key[0] == '/':\n",
    "            key = key[1:]\n",
    "        k = self.bucket.get_key(key)\n",
    "        if not k:\n",
    "            raise Exception(\n",
    "                \"File \" + src + \" doesn't exist.\\n\"\n",
    "                \"Note that the transfer between S3 and local filesystem \"\n",
    "                \"do not support directory.\")\n",
    "        k.get_contents_to_filename(tgt)\n",
    "\n",
    "    @staticmethod\n",
    "    def local_to_hdfs(src, tgt):\n",
    "        \"\"\"Upload local directory to HDFS.\n",
    "\n",
    "           Args:\n",
    "               src - path to the local directory,\n",
    "               tgt - path to the HDFS directory\n",
    "           Returns:\n",
    "               None\n",
    "        \"\"\"\n",
    "        if src[0] != '/' or tgt[0] != '/':\n",
    "            raise Exception(\"The directory path cannot be an relative path.\")\n",
    "        _local_to_hdfs(src, tgt)\n",
    "\n",
    "\n",
    "s3helper = S3Helper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        s3helper is a helper object to move files and directory between\n",
      "        local filesystem, AWS S3 and local HDFS.\n",
      "\n",
      "        Usage:\n",
      "\n",
      "        1. Open a S3 bucket under your account\n",
      "            s3helper.open_bucket(<bucket_name>)\n",
      "        2. List all files under the opened S3 bucket\n",
      "            s3helper.ls() or s3helper.ls_s3()\n",
      "        Or optionally,\n",
      "            s3helper.ls(<file_path>) or s3helper.ls_s3(<file_path>)\n",
      "        3. List all files on HDFS\n",
      "            s3helper.ls_hdfs()\n",
      "        Or optionally,\n",
      "            s3helper.ls_hdfs(<file_path>)\n",
      "        where <file_path> is an absolute path in the opened S3 bucket.\n",
      "\n",
      "        Now you can access your S3 files.\n",
      "\n",
      "        1. Transfer files between S3 and HDFS\n",
      "          a. To download all S3 files under a directory to HDFS, please call\n",
      "                s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)\n",
      "          b. To upload a directory on HDFS to S3, please call\n",
      "                s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)\n",
      "\n",
      "        2. Transfer files between S3 and local filesystem (not HDFS)\n",
      "          a. To download one single S3 file to local filesystem, please call\n",
      "                s3helper.s3_to_local(<s3_file_path>, <local_file_path>)\n",
      "          b. To upload a file on local filesystem to S3, please call\n",
      "                s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)\n",
      "\n",
      "        3. Transfer files between local filesystem and HDFS\n",
      "          a. To upload a directory on local filesystem to HDFS, please call\n",
      "                s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)\n",
      "\n",
      "        4. Get S3 file paths without data transfer\n",
      "          a. To get the URLs of S3 files under a directory, please call\n",
      "                s3helper.get_path(<s3_directory_path>)\n",
      "          Note this method do nothing on your local HDFS.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "s3helper.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Open the bucket that has your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: S3 region is not defined. Default region is set to 'us-east-1'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'256_STAT',\n",
       " u'ALL.csv.gz',\n",
       " u'ALLBootstrap.sh',\n",
       " u'MasterBootstrap.sh',\n",
       " u'NY.parquet',\n",
       " u'PrivateBootstrap.sh',\n",
       " u'RunFromTerminal.sh',\n",
       " u'US_Weather_with_smoothed.parquet',\n",
       " u'US_Weather_with_smoothed.parquet_$folder$',\n",
       " u'US_stations.parquet',\n",
       " u'US_weather.parquet',\n",
       " u'info',\n",
       " u's3hook.sh',\n",
       " u'weather.parquet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.open_bucket('dse-weather')\n",
    "s3helper.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'info/US_stations.tsv.gz',\n",
       " u'info/all_stations.tsv.gz',\n",
       " u'info/stations.parquet/_SUCCESS',\n",
       " u'info/stations.parquet/_common_metadata',\n",
       " u'info/stations.parquet/_metadata',\n",
       " u'info/stations.parquet/part-r-00000-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet',\n",
       " u'info/stations.parquet/part-r-00001-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3helper.open_bucket('dse-weather-west-2', region=\"us-west-2\")\n",
    "s3helper.ls('info/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) List files in the S3 bucket and HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aws\n",
      ".bash_profile\n",
      ".bashrc\n",
      ".conda\n",
      ".ipython\n",
      ".jupyter\n",
      ".local\n",
      ".ssh\n",
      "ALL.csv.gz\n",
      "ALLBootstrap.sh\n",
      "MasterBootstrap.sh\n",
      "NY.parquet\n",
      "PrivateBootstrap.sh\n",
      "RunFromTerminal.sh\n",
      "US_Weather_with_smoothed.parquet\n",
      "US_Weather_with_smoothed.parquet_$folder$\n",
      "US_stations.parquet\n",
      "US_weather.parquet\n",
      "fromLocal\n",
      "info\n",
      "s3helper.py\n",
      "s3hook.sh\n",
      "weather.parquet\n",
      "[]\n",
      "Found 3 items\n",
      "drwxrwxrwt   - hdfs hadoop          0 2020-04-28 00:01 /tmp\n",
      "drwxr-xr-x   - hdfs hadoop          0 2020-04-27 19:07 /user\n",
      "drwxr-xr-x   - hdfs hadoop          0 2020-04-27 19:07 /var\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(s3helper.ls_s3()))  # By default, list all files in the root directory of the bucket\n",
    "print(s3helper.ls_s3('fromHDFS'))\n",
    "\n",
    "print(s3helper.ls_hdfs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Move files around local filesystem, HDFS and S3\n",
    "\n",
    "As described in `s3helper.help()`, there are five methods for file transfers:\n",
    "\n",
    "1. `s3helper.s3_to_hdfs(<s3_directory_path>, <HDFS_directory_path>)`\n",
    "2. `s3helper.hdfs_to_s3(<HDFS_directory_path>, <s3_directory_path>)`\n",
    "3. `s3helper.s3_to_local(<s3_file_path>, <local_file_path>)`\n",
    "4. `s3helper.local_to_s3(<local_file_path>, <s3_directory_path>)`\n",
    "5. `s3helper.local_to_hdfs(<local_dir_path>, <HDFS_dir_path>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 hadoop hadoop 9409 Apr 27 18:58 /home/hadoop/s3helper.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l /home/hadoop/s3helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3helper.local_to_s3(\"/home/hadoop/s3helper.py\", \"fromLocal/s3helper.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'fromLocal/s3helper.py']\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_s3(\"fromLocal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/_SUCCESS' for reading\n",
      "cp: `/tmp/weather.parquet/_SUCCESS': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00000-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00000-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00001-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00001-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00002-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00002-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00003-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00003-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00004-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00004-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00005-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00005-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:12 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00006-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00006-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00007-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00007-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00008-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00008-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00009-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00009-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00010-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00010-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00011-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00011-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:13 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00012-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00012-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00013-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00013-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00014-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00014-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00015-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00015-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00016-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00016-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00017-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00017-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00018-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00018-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00019-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00019-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:14 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00020-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00020-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:16 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00021-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00021-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:16 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00022-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00022-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:16 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00023-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00023-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:16 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00024-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00024-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:17 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00025-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00025-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:17 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00026-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00026-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:17 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00027-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00027-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:17 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00028-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00028-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00029-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00029-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00030-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00030-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00031-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00031-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00032-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00032-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00033-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00033-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00034-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00034-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:18 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00035-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00035-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00036-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00036-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00037-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00037-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00038-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00038-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00039-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00039-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00040-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00040-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00041-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00041-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00042-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00042-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00043-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00043-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:19 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00044-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00044-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00045-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00045-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00046-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00046-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00047-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00047-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00048-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00048-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00049-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00049-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00050-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00050-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:20 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00051-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00051-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00052-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00052-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00053-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00053-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00054-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00054-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00055-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00055-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00056-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00056-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "20/04/28 00:18:21 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/weather.parquet/part-00057-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet' for reading\n",
      "cp: `/tmp/weather.parquet/part-00057-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet': File exists\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 59 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2020-04-28 00:01 /tmp/weather.parquet/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop   40670401 2020-04-28 00:01 /tmp/weather.parquet/part-00000-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40307528 2020-04-28 00:01 /tmp/weather.parquet/part-00001-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40016618 2020-04-28 00:01 /tmp/weather.parquet/part-00002-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40377232 2020-04-28 00:01 /tmp/weather.parquet/part-00003-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40119938 2020-04-28 00:01 /tmp/weather.parquet/part-00004-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40278884 2020-04-28 00:01 /tmp/weather.parquet/part-00005-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40078149 2020-04-28 00:01 /tmp/weather.parquet/part-00006-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40838598 2020-04-28 00:01 /tmp/weather.parquet/part-00007-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40257133 2020-04-28 00:01 /tmp/weather.parquet/part-00008-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40970219 2020-04-28 00:01 /tmp/weather.parquet/part-00009-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39737699 2020-04-28 00:01 /tmp/weather.parquet/part-00010-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40588195 2020-04-28 00:01 /tmp/weather.parquet/part-00011-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39670806 2020-04-28 00:01 /tmp/weather.parquet/part-00012-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40265820 2020-04-28 00:02 /tmp/weather.parquet/part-00013-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40504283 2020-04-28 00:02 /tmp/weather.parquet/part-00014-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39839877 2020-04-28 00:02 /tmp/weather.parquet/part-00015-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39666134 2020-04-28 00:02 /tmp/weather.parquet/part-00016-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39971356 2020-04-28 00:02 /tmp/weather.parquet/part-00017-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40604360 2020-04-28 00:02 /tmp/weather.parquet/part-00018-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40758812 2020-04-28 00:02 /tmp/weather.parquet/part-00019-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40382015 2020-04-28 00:02 /tmp/weather.parquet/part-00020-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40181667 2020-04-28 00:02 /tmp/weather.parquet/part-00021-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40236103 2020-04-28 00:02 /tmp/weather.parquet/part-00022-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40395446 2020-04-28 00:02 /tmp/weather.parquet/part-00023-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39821857 2020-04-28 00:02 /tmp/weather.parquet/part-00024-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40418247 2020-04-28 00:02 /tmp/weather.parquet/part-00025-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40050460 2020-04-28 00:02 /tmp/weather.parquet/part-00026-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40298252 2020-04-28 00:02 /tmp/weather.parquet/part-00027-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40702652 2020-04-28 00:02 /tmp/weather.parquet/part-00028-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40528929 2020-04-28 00:02 /tmp/weather.parquet/part-00029-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40157633 2020-04-28 00:02 /tmp/weather.parquet/part-00030-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40437074 2020-04-28 00:02 /tmp/weather.parquet/part-00031-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40382705 2020-04-28 00:02 /tmp/weather.parquet/part-00032-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39730289 2020-04-28 00:02 /tmp/weather.parquet/part-00033-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40013953 2020-04-28 00:02 /tmp/weather.parquet/part-00034-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39831970 2020-04-28 00:02 /tmp/weather.parquet/part-00035-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40882114 2020-04-28 00:02 /tmp/weather.parquet/part-00036-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39763515 2020-04-28 00:02 /tmp/weather.parquet/part-00037-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40065826 2020-04-28 00:02 /tmp/weather.parquet/part-00038-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40247715 2020-04-28 00:02 /tmp/weather.parquet/part-00039-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40742694 2020-04-28 00:02 /tmp/weather.parquet/part-00040-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40173691 2020-04-28 00:02 /tmp/weather.parquet/part-00041-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40461662 2020-04-28 00:02 /tmp/weather.parquet/part-00042-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40953050 2020-04-28 00:02 /tmp/weather.parquet/part-00043-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40845591 2020-04-28 00:02 /tmp/weather.parquet/part-00044-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40107437 2020-04-28 00:02 /tmp/weather.parquet/part-00045-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39649676 2020-04-28 00:02 /tmp/weather.parquet/part-00046-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39912209 2020-04-28 00:02 /tmp/weather.parquet/part-00047-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   41212447 2020-04-28 00:02 /tmp/weather.parquet/part-00048-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   39871083 2020-04-28 00:02 /tmp/weather.parquet/part-00049-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40465828 2020-04-28 00:02 /tmp/weather.parquet/part-00050-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40511437 2020-04-28 00:02 /tmp/weather.parquet/part-00051-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40578221 2020-04-28 00:02 /tmp/weather.parquet/part-00052-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40458876 2020-04-28 00:02 /tmp/weather.parquet/part-00053-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40108325 2020-04-28 00:02 /tmp/weather.parquet/part-00054-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40518066 2020-04-28 00:02 /tmp/weather.parquet/part-00055-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop   40574923 2020-04-28 00:02 /tmp/weather.parquet/part-00056-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "-rw-r--r--   2 hadoop hadoop    5605101 2020-04-28 00:02 /tmp/weather.parquet/part-00057-6cb19187-62a0-42ad-9516-e03e05ea0c40-c000.snappy.parquet\n",
      "\n",
      "CPU times: user 24 ms, sys: 16 ms, total: 40 ms\n",
      "Wall time: 19.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s3helper.s3_to_hdfs(\"weather.parquet\", \"/tmp/weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir /home/hadoop/fromS3\n",
    "mkdir /home/hadoop/fromHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/04/28 00:24:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_SUCCESS' for reading\n",
      "20/04/28 00:24:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_common_metadata' for reading\n",
      "20/04/28 00:24:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/_metadata' for reading\n",
      "20/04/28 00:24:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/part-r-00000-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet' for reading\n",
      "20/04/28 00:24:44 INFO s3n.S3NativeFileSystem: Opening 's3n://dse-weather-west-2/info/stations.parquet/part-r-00001-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet' for reading\n",
      "\n",
      "Found 5 items\n",
      "-rw-r--r--   2 hadoop hadoop          0 2020-04-28 00:24 /tmp/stations.parquet/_SUCCESS\n",
      "-rw-r--r--   2 hadoop hadoop        894 2020-04-28 00:24 /tmp/stations.parquet/_common_metadata\n",
      "-rw-r--r--   2 hadoop hadoop       3107 2020-04-28 00:24 /tmp/stations.parquet/_metadata\n",
      "-rw-r--r--   2 hadoop hadoop     777171 2020-04-28 00:24 /tmp/stations.parquet/part-r-00000-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet\n",
      "-rw-r--r--   2 hadoop hadoop     860566 2020-04-28 00:24 /tmp/stations.parquet/part-r-00001-1fd04699-91d3-4a2a-9b36-e25c9c5f0376.gz.parquet\n",
      "\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "s3helper.s3_to_hdfs(\"info/stations.parquet\", \"/tmp/stations.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxrwxrwx   - mapred mapred          0 2020-04-27 19:07 /tmp/hadoop-yarn\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-04-28 00:24 /tmp/stations.parquet\n",
      "drwxr-xr-x   - hadoop hadoop          0 2020-04-28 00:02 /tmp/weather.parquet\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(s3helper.ls_hdfs(\"/tmp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Files\n",
    "\n",
    "To get a reasonable reading speed, please always load parquet files from S3 to HDFS before accessing them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "s3helper.open_bucket(\"your-bucket-name\")\n",
    "\n",
    "files = s3helper.s3_to_hdfs('/sub-directory-for-parquets', '/hdfs-directory.parquet')\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = sqlContext.sql(\"SELECT * FROM parquet.`/tmp/weather.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stations=sqlContext.sql(\"SELECT * FROM parquet.`/tmp/stations.parquet`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9358394"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+\n",
      "|    Station|Measurement|Year|              Values|\n",
      "+-----------+-----------+----+--------------------+\n",
      "|USW00093819|       WDFM|1972|[38 58 EC 5C 38 5...|\n",
      "|USW00093819|       WDFM|1973|[38 5C EC 5C 08 5...|\n",
      "|USW00093819|       WDFM|1974|[EC 5C A0 51 EC 5...|\n",
      "|USW00093819|       WDFM|1975|[EC 5C 38 58 38 5...|\n",
      "|USW00093819|       WDFM|1976|[38 58 38 58 38 5...|\n",
      "|USW00093819|       WDFM|1977|[EC 5C 38 5C A0 5...|\n",
      "|USW00093819|       WDFM|1978|[38 5C 08 5B EC 5...|\n",
      "|USW00093819|       WDFM|1979|[EC 5C 38 5C 08 5...|\n",
      "|USW00093819|       WESD|1952|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1953|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1954|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1955|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1956|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1957|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1958|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1959|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1960|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1961|[00 7E 00 7E 00 7...|\n",
      "|USW00093819|       WESD|1962|[A0 53 A0 53 60 5...|\n",
      "|USW00093819|       WESD|1963|[00 7E 00 7E 00 7...|\n",
      "+-----------+-----------+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+---------+---------+-----+-------------+-------+-------+-----+\n",
      "|         ID|latitude|longitude|elevation|state|         name|GSNFLAG|HCNFLAG|WMOID|\n",
      "+-----------+--------+---------+---------+-----+-------------+-------+-------+-----+\n",
      "|US1COLR0185|  40.711|-105.1144|   1599.0|   CO|WELLINGTON 5.|       |       |     |\n",
      "|US1COLR0186| 40.8135|-105.0963|   1601.7|   CO|BUCKEYE 0.9 S|       |       |     |\n",
      "|US1COLR0187| 40.7689| -105.064|   1653.8|   CO|WELLINGTON 5.|       |       |     |\n",
      "|US1COLR0189|  40.689|-105.0242|   1594.1|   CO|WELLINGTON 1.|       |       |     |\n",
      "|US1COLR0193| 40.6711|-105.0639|   1584.0|   CO|WELLINGTON 3.|       |       |     |\n",
      "|US1COLR0196|  40.691|-105.0157|   1581.9|   CO|WELLINGTON 0.|       |       |     |\n",
      "|US1COLR0197|  40.625|-105.3403|   2464.0|   CO|    BLV 8.0 W|       |       |     |\n",
      "|US1COLR0200| 40.3345|-105.5127|   2431.1|   CO|ESTES PARK 2.|       |       |     |\n",
      "|US1COLR0201| 40.4701|-105.4493|   2303.1|   CO|GLEN HAVEN 1.|       |       |     |\n",
      "|US1COLR0202| 40.3588|-105.5635|   2389.9|   CO|ESTES PARK 2.|       |       |     |\n",
      "|US1COLR0205| 40.4898|-105.5015|   2567.9|   CO| GLEN HAVEN 4|       |       |     |\n",
      "|US1COLR0206|  40.456|-105.4566|   2243.9|   CO|GLEN HAVEN 0.|       |       |     |\n",
      "|US1COLR0208| 40.8103|-105.9351|   2523.1|   CO|          GLN|       |       |     |\n",
      "|US1COLR0209| 40.3775|-105.5331|   2318.0|   CO|ESTES PARK 0.|       |       |     |\n",
      "|US1COLR0210| 40.4708|-105.4364|   2222.9|   CO|GLEN HAVEN 1.|       |       |     |\n",
      "|US1COLR0211| 40.4703| -105.428|   2250.0|   CO|GLEN HAVEN 1.|       |       |     |\n",
      "|US1COLR0212| 40.3606|-105.5304|   2418.9|   CO|ESTES PARK 1.|       |       |     |\n",
      "|US1COLR0213| 40.3551|-105.4937|   2361.9|   CO|ESTES PARK 1.|       |       |     |\n",
      "|US1COLR0214| 40.3395|-105.5605|   2567.9|   CO|ESTES PARK 3.|       |       |     |\n",
      "|US1COLR0215| 40.3571|-105.5123|   2425.0|   CO|ESTES PARK 1.|       |       |     |\n",
      "+-----------+--------+---------+---------+-----+-------------+-------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_names=stations.select('ID').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85284"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(st_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_names1=[r.ID for r in st_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'US1COLR0171',\n",
       " u'US1COLR0172',\n",
       " u'US1COLR0173',\n",
       " u'US1COLR0181',\n",
       " u'US1COLR0183']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_names1[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=Counter([x[:3] for x in st_names1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'USC', 22144),\n",
       " (u'US1', 20054),\n",
       " (u'ASN', 17081),\n",
       " (u'BR0', 5934),\n",
       " (u'CA0', 5255),\n",
       " (u'IN0', 3805),\n",
       " (u'USW', 1731),\n",
       " (u'USR', 1454),\n",
       " (u'SF0', 1154),\n",
       " (u'RSM', 1096),\n",
       " (u'USS', 668),\n",
       " (u'NOE', 399),\n",
       " (u'NLE', 353),\n",
       " (u'KZ0', 328),\n",
       " (u'WA0', 281),\n",
       " (u'MX0', 220),\n",
       " (u'UPM', 204),\n",
       " (u'CH0', 198),\n",
       " (u'JA0', 154),\n",
       " (u'RQC', 148),\n",
       " (u'UY0', 146),\n",
       " (u'GG0', 102),\n",
       " (u'SPE', 99),\n",
       " (u'VE0', 81),\n",
       " (u'UZM', 78),\n",
       " (u'KG0', 73),\n",
       " (u'GME', 69),\n",
       " (u'AJ0', 66),\n",
       " (u'TI0', 62),\n",
       " (u'TX0', 57),\n",
       " (u'AM0', 52),\n",
       " (u'BOM', 51),\n",
       " (u'KSW', 45),\n",
       " (u'TH0', 45),\n",
       " (u'FRE', 44),\n",
       " (u'VMW', 42),\n",
       " (u'GMW', 40),\n",
       " (u'AR0', 40),\n",
       " (u'SWE', 40),\n",
       " (u'JAW', 39),\n",
       " (u'VQC', 37),\n",
       " (u'LG0', 32),\n",
       " (u'FMC', 29),\n",
       " (u'CA1', 28),\n",
       " (u'EN0', 25),\n",
       " (u'ROE', 25),\n",
       " (u'UKW', 24),\n",
       " (u'LH0', 22),\n",
       " (u'GM0', 22),\n",
       " (u'ZI0', 20),\n",
       " (u'NO0', 20),\n",
       " (u'BC0', 20),\n",
       " (u'SIE', 19),\n",
       " (u'MI0', 19),\n",
       " (u'AQC', 19),\n",
       " (u'ZA0', 18),\n",
       " (u'MD0', 18),\n",
       " (u'ET0', 18),\n",
       " (u'SW0', 18),\n",
       " (u'MR0', 17),\n",
       " (u'ID0', 17),\n",
       " (u'SU0', 16),\n",
       " (u'GB0', 16),\n",
       " (u'SP0', 16),\n",
       " (u'GQC', 15),\n",
       " (u'AYW', 15),\n",
       " (u'CD0', 15),\n",
       " (u'IV0', 15),\n",
       " (u'GLW', 15),\n",
       " (u'NG0', 14),\n",
       " (u'FR0', 14),\n",
       " (u'CT0', 14),\n",
       " (u'TZ0', 13),\n",
       " (u'CF0', 13),\n",
       " (u'FRW', 12),\n",
       " (u'PK0', 12),\n",
       " (u'UK0', 12),\n",
       " (u'RMW', 11),\n",
       " (u'RPW', 11),\n",
       " (u'CI0', 11),\n",
       " (u'IT0', 10),\n",
       " (u'MG0', 10),\n",
       " (u'LUE', 10),\n",
       " (u'RMC', 10),\n",
       " (u'NZ0', 10),\n",
       " (u'RP0', 10),\n",
       " (u'MZ0', 9),\n",
       " (u'LAW', 9),\n",
       " (u'UV0', 9),\n",
       " (u'FMW', 9),\n",
       " (u'DA0', 9),\n",
       " (u'IR0', 8),\n",
       " (u'AY0', 8),\n",
       " (u'CQC', 8),\n",
       " (u'POE', 8),\n",
       " (u'BL0', 8),\n",
       " (u'UG0', 8),\n",
       " (u'THW', 8),\n",
       " (u'PSC', 8),\n",
       " (u'GV0', 8),\n",
       " (u'PO0', 8),\n",
       " (u'ITE', 8),\n",
       " (u'MA0', 7),\n",
       " (u'BN0', 7),\n",
       " (u'CYE', 7),\n",
       " (u'HRE', 7),\n",
       " (u'PA0', 7),\n",
       " (u'GR0', 7),\n",
       " (u'KE0', 7),\n",
       " (u'GRE', 7),\n",
       " (u'SZ0', 7),\n",
       " (u'TU0', 7),\n",
       " (u'FP0', 7),\n",
       " (u'SG0', 7),\n",
       " (u'PMW', 6),\n",
       " (u'MY0', 6),\n",
       " (u'AO0', 6),\n",
       " (u'RQW', 6),\n",
       " (u'EG0', 6),\n",
       " (u'NL0', 6),\n",
       " (u'MOW', 6),\n",
       " (u'WZ0', 6),\n",
       " (u'AU0', 6),\n",
       " (u'IC0', 6),\n",
       " (u'BFW', 5),\n",
       " (u'SA0', 5),\n",
       " (u'JAC', 5),\n",
       " (u'IS0', 5),\n",
       " (u'MP0', 5),\n",
       " (u'KRW', 5),\n",
       " (u'EC0', 5),\n",
       " (u'CO0', 5),\n",
       " (u'PE0', 5),\n",
       " (u'TO0', 5),\n",
       " (u'GQW', 5),\n",
       " (u'ML0', 5),\n",
       " (u'HO0', 4),\n",
       " (u'FI0', 4),\n",
       " (u'SPW', 4),\n",
       " (u'ITW', 4),\n",
       " (u'AG0', 4),\n",
       " (u'ISE', 4),\n",
       " (u'CS0', 4),\n",
       " (u'NU0', 4),\n",
       " (u'LT0', 4),\n",
       " (u'PM0', 4),\n",
       " (u'JQW', 4),\n",
       " (u'DAE', 4),\n",
       " (u'EI0', 4),\n",
       " (u'SY0', 3),\n",
       " (u'RIE', 3),\n",
       " (u'LY0', 3),\n",
       " (u'KNW', 3),\n",
       " (u'CQW', 3),\n",
       " (u'MO0', 3),\n",
       " (u'RO0', 3),\n",
       " (u'FJ0', 3),\n",
       " (u'KS0', 3),\n",
       " (u'PSW', 3),\n",
       " (u'EIE', 3),\n",
       " (u'GLE', 3),\n",
       " (u'GL0', 3),\n",
       " (u'LOE', 3),\n",
       " (u'CAW', 3),\n",
       " (u'RS0', 3),\n",
       " (u'ES0', 3),\n",
       " (u'PP0', 3),\n",
       " (u'ER0', 3),\n",
       " (u'CHW', 2),\n",
       " (u'BY0', 2),\n",
       " (u'SV0', 2),\n",
       " (u'LQW', 2),\n",
       " (u'VQW', 2),\n",
       " (u'TUW', 2),\n",
       " (u'SH0', 2),\n",
       " (u'SZE', 2),\n",
       " (u'TDW', 2),\n",
       " (u'CE0', 2),\n",
       " (u'STW', 2),\n",
       " (u'TS0', 2),\n",
       " (u'CM0', 2),\n",
       " (u'PL0', 2),\n",
       " (u'ACW', 2),\n",
       " (u'KR0', 2),\n",
       " (u'CUW', 2),\n",
       " (u'MU0', 2),\n",
       " (u'ALE', 2),\n",
       " (u'NC0', 2),\n",
       " (u'TV0', 2),\n",
       " (u'HOW', 2),\n",
       " (u'NH0', 2),\n",
       " (u'ICW', 2),\n",
       " (u'BP0', 2),\n",
       " (u'JN0', 1),\n",
       " (u'SL0', 1),\n",
       " (u'AGE', 1),\n",
       " (u'DRW', 1),\n",
       " (u'JMW', 1),\n",
       " (u'NP0', 1),\n",
       " (u'LQC', 1),\n",
       " (u'IZ0', 1),\n",
       " (u'SIM', 1),\n",
       " (u'IOW', 1),\n",
       " (u'HR0', 1),\n",
       " (u'BF0', 1),\n",
       " (u'MQW', 1),\n",
       " (u'GP0', 1),\n",
       " (u'FG0', 1),\n",
       " (u'WQW', 1),\n",
       " (u'CK0', 1),\n",
       " (u'TL0', 1),\n",
       " (u'BDM', 1),\n",
       " (u'AF0', 1),\n",
       " (u'GRW', 1),\n",
       " (u'ENE', 1),\n",
       " (u'SHW', 1),\n",
       " (u'JM0', 1),\n",
       " (u'MQC', 1),\n",
       " (u'BDW', 1),\n",
       " (u'HU0', 1),\n",
       " (u'CJW', 1),\n",
       " (u'AE0', 1),\n",
       " (u'PC0', 1),\n",
       " (u'BE0', 1),\n",
       " (u'GT0', 1),\n",
       " (u'VM0', 1),\n",
       " (u'MV0', 1),\n",
       " (u'BKE', 1),\n",
       " (u'NUW', 1),\n",
       " (u'ECW', 1),\n",
       " (u'GGM', 1),\n",
       " (u'EZ0', 1),\n",
       " (u'BK0', 1),\n",
       " (u'AUW', 1),\n",
       " (u'KRC', 1),\n",
       " (u'LU0', 1),\n",
       " (u'BRW', 1),\n",
       " (u'SAW', 1),\n",
       " (u'KU0', 1),\n",
       " (u'FS0', 1),\n",
       " (u'BB0', 1),\n",
       " (u'RPC', 1),\n",
       " (u'MKE', 1),\n",
       " (u'LYW', 1),\n",
       " (u'CIW', 1),\n",
       " (u'BPW', 1),\n",
       " (u'NCW', 1),\n",
       " (u'AQW', 1),\n",
       " (u'TN0', 1),\n",
       " (u'POW', 1),\n",
       " (u'MT0', 1),\n",
       " (u'SE0', 1),\n",
       " (u'KT0', 1),\n",
       " (u'GYW', 1),\n",
       " (u'PPW', 1),\n",
       " (u'CY0', 1),\n",
       " (u'WF0', 1),\n",
       " (u'BA0', 1),\n",
       " (u'LGE', 1),\n",
       " (u'LO0', 1),\n",
       " (u'MK0', 1),\n",
       " (u'EZE', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(C.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
